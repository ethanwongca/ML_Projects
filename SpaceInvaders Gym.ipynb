{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SpaceInvaders Gym\n",
    "*If code differs between the notebook and the curriculum, go with the notebook!!!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import pickle\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the environment \n",
    "### & setup Q and other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SpaceInvaders-v0')\n",
    "state = env.reset()\n",
    "action_space = env.action_space.n\n",
    "state_space = (90, 70, 4)\n",
    "\n",
    "possible_actions = np.array(np.identity(action_space, dtype=int).tolist())\n",
    "\n",
    "stack_size=4\n",
    "\n",
    "learning_rate = 0.00025\n",
    "decay_rate = 0.00001\n",
    "eps_max = 1\n",
    "eps_min = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frame preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_frame(frame):\n",
    "    cropped_downsampled = frame[20:-10:2, 10:-10:2]\n",
    "    return np.mean(cropped_downsampled, axis=2).astype(np.uint8) / 255\n",
    "\n",
    "def stack_frames(stacked_frames, new_frame, new_episode):\n",
    "    stacked_state = None\n",
    "    frame = preprocess_frame(new_frame)\n",
    "    if new_episode:\n",
    "        stacked_frames = deque([np.zeros((90, 70), dtype=np.uint8) for i in range(stack_size)], maxlen=4) # maxlen missing in curric\n",
    "        for _ in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "            stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis=2)\n",
    "    return stacked_state, stacked_frames\n",
    "\n",
    "def greedy_action(model, decay_step):\n",
    "    exp_tradeoff = np.random.rand()\n",
    "    epsilon = eps_min + (eps_max - eps_min) * np.exp(-decay_rate * decay_step)\n",
    "    if (epsilon > exp_tradeoff):\n",
    "        choice = np.random.randint(action_space)\n",
    "        action = possible_actions[choice]\n",
    "    else:\n",
    "        choice = np.argmax(model.predict(np.array(stacked_frames).reshape(1, *state_space)))\n",
    "        action = possible_actions[choice]\n",
    "    return action\n",
    "\n",
    "def sample_memory(buffered_list, batch_size):\n",
    "    buffer_size = len(buffered_list)\n",
    "    index = np.random.choice(np.arange(buffer_size), size=batch_size, replace=False)\n",
    "    return [buffered_list[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the model\n",
    "model = Sequential([\n",
    "        Conv2D(16, (8,8), strides=(4,4), data_format=\"channels_last\", input_shape=(90,70,4), activation='relu'),\n",
    "        Conv2D(32, (4,4), strides=(2,2), data_format=\"channels_last\", activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(action_space, activation='softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=learning_rate,\n",
    "                                              beta_1=eps_min,\n",
    "                                              beta_2=eps_max,\n",
    "                                              decay=decay_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The training algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = []\n",
    "\n",
    "memory = deque(maxlen=1000)\n",
    "\n",
    "stacked_frames = deque([np.zeros((90, 70), dtype=np.uint8) for i in range(stack_size)], maxlen=4)\n",
    "\n",
    "rewards_list = []\n",
    "\n",
    "decay_step = 0\n",
    "\n",
    "batch_size=60\n",
    "\n",
    "for episode in range(100):\n",
    "    \n",
    "    obs = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    obs, stacked_frames = stack_frames(stacked_frames, obs, True)\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "\n",
    "        decay_step += 1\n",
    "        \n",
    "        action = np.argmax(greedy_action(model, decay_step))\n",
    "        \n",
    "        next_obs, reward, done, _ = env.step(action)\n",
    "        \n",
    "        next_obs, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
    "        memory.append((obs, action, reward, next_obs, done))\n",
    "        \n",
    "        # we'll add the reward to our existing total_reward\n",
    "        total_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            rewards_list.append(total_reward)\n",
    "            \n",
    "\n",
    "        obs = next_obs\n",
    "        \n",
    "\n",
    "    if len(memory) > 100:\n",
    "\n",
    "        batch = sample_memory(memory, batch_size=batch_size)\n",
    "        states = np.array([item[0] for item in batch], ndmin=3)\n",
    "        actions = [item[1] for item in batch]\n",
    "        rewards = [item[2] for item in batch]\n",
    "        next_states = np.array([item[3] for item in batch], ndmin=3)\n",
    "        \n",
    "\n",
    "        targets = [learning_rate * np.max(item) for item in model.predict(next_states)]\n",
    "        targets = [targets[i] + rewards[i] for i in range(len(targets))]\n",
    "        \n",
    "        # creates the outputs to fit to\n",
    "        target_f = [item for item in model.predict(states)]\n",
    "        for i in range(len(target_f)):\n",
    "            target_f[i][actions[i]] = targets[i]\n",
    "            \n",
    "        # train on whole batch!\n",
    "        model.train_on_batch(x=np.array(states).reshape(-1, * state_space),\n",
    "                                            y=np.array(target_f).reshape(-1, action_space))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving?\n",
    "After all that, you might want to save your model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pickle.dump(model, open('SpaceInvadersModel.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "model = pickle.load(open('SpaceInvadersModel.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the way we've written this notebook is not necessarily best practice... OOP would make for more robust, reusable code. Extension (and more emphasis in general) could be about building out classes. Also, missing reliable and regular validation tests due to train_on_batch... need to monitor training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing\n",
    "This one includes a rendering option. By default it shows the game play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def play_game(num_games, model=False, render=True):\n",
    "    \n",
    "    stacked_frames = deque([np.zeros((90, 70), dtype=np.uint8) for i in range(stack_size)], maxlen=4)\n",
    "    t_score = 0\n",
    "    \n",
    "    for episode in range(num_games):\n",
    "        \n",
    "        done = False\n",
    "        turn = 0\n",
    "        score = 0\n",
    "        obs = env.reset()\n",
    "        \n",
    "        obs, stacked_frames = stack_frames(stacked_frames, obs, True)\n",
    "\n",
    "        while not done:\n",
    "            \n",
    "            if render:\n",
    "                env.render()\n",
    "\n",
    "            if model:\n",
    "                # get our move from the model\n",
    "                choice = np.argmax(model.predict(np.array(stacked_frames).reshape(1, *state_space)))\n",
    "                action = np.argmax(possible_actions[choice])\n",
    "                next_obs, reward, done, _ = env.step(action)  \n",
    "                \n",
    "                # stack it \n",
    "                next_obs, stacked_frames = stack_frames(stacked_frames, next_obs, False)\n",
    "                state = next_obs\n",
    "            else:\n",
    "                # random agent\n",
    "                _, reward, done, _ = env.step(env.action_space.sample())\n",
    "                \n",
    "            # tally score\n",
    "            score += reward\n",
    "\n",
    "        if render:\n",
    "            env.close()\n",
    "        \n",
    "        print(f'Game {episode + 1} score: {score}')\n",
    "            \n",
    "        # get avg score for all games played... for rough analysis\n",
    "        t_score += score\n",
    "\n",
    "    print(f'Average score over {num_games} games: {t_score/num_games}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 score: 145.0\n",
      "Game 2 score: 215.0\n",
      "Game 3 score: 290.0\n",
      "Game 4 score: 90.0\n",
      "Game 5 score: 205.0\n",
      "Average score over 5 games: 189.0\n"
     ]
    }
   ],
   "source": [
    "play_game(5, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
